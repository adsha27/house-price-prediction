{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - California Housing Dataset\n",
    "\n",
    "**Objective:** This notebook performs an initial exploratory data analysis on the California Housing dataset. The goals are:\n",
    "1.  Understand the structure, data types, and statistical properties of the dataset.\n",
    "2.  Analyze the distribution of the target variable, `MedHouseVal`.\n",
    "3.  Visualize the distributions of individual features.\n",
    "4.  Investigate relationships and correlations between features, especially with the target variable.\n",
    "5.  Identify potential data quality issues, outliers, or patterns that will inform feature engineering and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import necessary libraries and set plotting styles for consistent and professional visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a professional plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "As per our project's architecture, we will not place data loading logic directly into the notebook. Instead, we import and use the `CaliforniaHousingLoader` from our `src` directory. This promotes code reuse, testability, and separation of concernsâ€”a key practice for production-level ML systems.\n",
    "\n",
    "We need to add the `src` directory to our system path to make the modules importable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project's root directory to the Python path to allow for `src` imports\n",
    "# This path is relative to the notebook's location: notebooks/01_data_exploration/\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data.loaders import CaliforniaHousingLoader\n",
    "\n",
    "try:\n",
    "    # Instantiate and load the data\n",
    "    loader = CaliforniaHousingLoader()\n",
    "    housing_df = loader.load()\n",
    "    print(\"Data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Inspection\n",
    "\n",
    "Let's perform a first-pass inspection of the DataFrame to understand its basic characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the dataframe\n",
    "# This is crucial for checking data types and missing values.\n",
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Observations from `.info()`:**\n",
    "- The dataset contains 20,640 entries.\n",
    "- There are 8 features and 1 target variable (`MedHouseVal`).\n",
    "- All columns are `float64`, which is expected for this dataset.\n",
    "- **Crucially, there are no missing values.** This simplifies our preprocessing, but in a real-world scenario, we would need a strategy for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from `.describe()`:**\n",
    "- The scales of the features vary widely (e.g., `MedInc` is in single digits, while `Population` is in thousands). This suggests that **feature scaling will be essential** for distance-based algorithms (like SVMs) and algorithms that use regularization (like Ridge/Lasso).\n",
    "- `HousingMedianAge` and the target `MedHouseVal` have a max value (52.0 and 5.00001 respectively) that seems to be a cap. This might be an artifact of data collection and could impact model performance. We should investigate this further.\n",
    "- The `AveRooms`, `AveBedrms`, `Population`, and `AveOccup` features have a large difference between their 75th percentile and max values, indicating the presence of outliers or a highly skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis (`MedHouseVal`)\n",
    "\n",
    "Understanding the distribution of the target variable is one of the most important steps in any regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(housing_df['MedHouseVal'], kde=True, bins=50)\n",
    "plt.title('Distribution of Median House Value')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=housing_df['MedHouseVal'])\n",
    "plt.title('Box Plot of Median House Value')\n",
    "plt.xlabel('Median House Value (,000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Target Variable:**\n",
    "- The distribution is **right-skewed**.\n",
    "- There is a clear **capping** at the maximum value of 5. This is a significant data artifact. The model may struggle to predict prices for houses in this top tier. Potential strategies could be to remove these samples if they are considered outliers or to treat this as a form of censorship in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution Analysis (Univariate)\n",
    "\n",
    "Let's visualize the distributions of all numerical features to understand their individual characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.hist(bins=50, figsize=(20, 15))\n",
    "plt.suptitle('Histograms of All Numerical Features', y=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Feature Distributions:**\n",
    "- **Skewness:** `MedInc`, `AveRooms`, `AveBedrms`, `Population`, and `AveOccup` are all right-skewed. This is common for features representing counts or monetary values. A log transformation might help to make their distributions more normal, which can be beneficial for some linear models.\n",
    "- **Capping:** `HousingMedianAge` is also clearly capped at 52 years.\n",
    "- **Bimodality:** `Latitude` and `Longitude` show multiple peaks, which makes sense as they represent geographical locations with population clusters (e.g., Los Angeles and Bay Area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Correlation Analysis\n",
    "\n",
    "Now we investigate the relationships between features, particularly how each feature correlates with the target variable, `MedHouseVal`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing_df.corr()\n",
    "\n",
    "# Focus on correlations with the target variable\n",
    "print(corr_matrix['MedHouseVal'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full correlation matrix with a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Housing Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Correlations:**\n",
    "- **`MedInc` (Median Income)** has the strongest positive correlation with `MedHouseVal` (0.69). This is highly intuitive; higher income areas tend to have more expensive houses.\n",
    "- **`AveRooms`** has a weak positive correlation (0.15).\n",
    "- **`Latitude`** has a slight negative correlation (-0.14), suggesting that houses in the north are slightly cheaper, though this is a very weak signal on its own.\n",
    "- There is a high correlation between `AveRooms` and `AveBedrms` (0.85), which indicates multicollinearity. This might be an issue for interpreting coefficients in linear models, but less so for tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Geospatial Analysis\n",
    "\n",
    "Since we have latitude and longitude data, we can create a geographical scatter plot to visualize housing prices across California."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.4,\n",
    "                s=housing_df[\"Population\"]/100, label=\"Population\", figsize=(12,9),\n",
    "                c=\"MedHouseVal\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "                sharex=False)\n",
    "plt.title(\"California Housing Prices and Population Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Geospatial Plot:**\n",
    "- The plot clearly resembles a map of California.\n",
    "- **High-priced areas (red/yellow)** are concentrated along the coast, particularly in the Bay Area (around Longitude -122) and Southern California (around Los Angeles and San Diego).\n",
    "- Inland areas generally have lower prices (blue/green).\n",
    "- This visualization confirms that **location is a critical factor** in determining house prices. This suggests that creating location-based features (e.g., distance to coast, clustering of districts) could be highly beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary of Findings & Next Steps\n",
    "\n",
    "This initial EDA has provided several key insights that will guide the next phase of the project.\n",
    "\n",
    "### Key Findings:\n",
    "1.  **Data Quality:** The data is clean with no missing values, but features have varying scales.\n",
    "2.  **Target Variable:** `MedHouseVal` is right-skewed and capped at \,000. This capping is a major characteristic to address.\n",
    "3.  **Feature Skewness:** Several important features are highly skewed (`MedInc`, `AveRooms`, etc.).\n",
    "4.  **Key Predictor:** `MedInc` is the strongest linear predictor of `MedHouseVal`.\n",
    "5.  **Location Importance:** Geospatial data shows that prices are heavily dependent on location, especially proximity to the coast.\n",
    "6.  **Multicollinearity:** `AveRooms` and `AveBedrms` are highly correlated.\n",
    "\n",
    "### Proposed Next Steps (for Feature Engineering):\n",
    "1.  **Transformation:** Apply log transformations to the skewed features (e.g., `MedInc`, `Population`, `AveRooms`, `AveOccup`) to normalize their distributions.\n",
    "2.  **Feature Scaling:** Standardize or normalize all features to bring them to a common scale, which is essential for many ML algorithms.\n",
    "3.  **Feature Creation:**\n",
    "    - Create new combination features, such as `rooms_per_person` (`TotalRooms` / `Population`) or `bedrooms_per_room` (`TotalBedrms` / `TotalRooms`).\n",
    "    - Create a categorical feature by clustering `Latitude` and `Longitude` to capture regional price differences.\n",
    "4.  **Handling Capped Values:** Decide on a strategy for the capped `MedHouseVal`. Options include:\n",
    "    a) Removing these instances before training.\n",
    "    b) Leaving them as is, but being aware that the model will not be able to predict values above the cap.\n",
    "    c) Treating it as a classification problem for that price bracket (more complex).\n",
    "\n",
    "This structured analysis provides a clear path forward for the `02_feature_engineering` phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}