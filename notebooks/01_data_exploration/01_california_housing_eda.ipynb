{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA) - California Housing Dataset\n",
    "\n",
    "**Objective:** This notebook performs an initial exploratory data analysis on the California Housing dataset. The goals are:\n",
    "1.  Understand the structure, data types, and statistical properties of the dataset.\n",
    "2.  Analyze the distribution of the target variable, `price`.\n",
    "3.  Visualize the distributions of individual features.\n",
    "4.  Investigate relationships and correlations between features, especially with the target variable.\n",
    "5.  Identify potential data quality issues, outliers, or patterns that will inform feature engineering and model selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Import necessary libraries and set plotting styles for consistent and professional visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set a professional plot style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"talk\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n",
    "\n",
    "**Reasoning:** As per our project's architecture, we will not place data loading logic directly into the notebook. Instead, we import and use the `CaliforniaHousingLoader` from our `src` directory. This promotes code reuse, testability, and separation of concernsâ€”a key practice for production-level ML systems.\n",
    "\n",
    "We need to add the `src` directory to our system path to make the modules importable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the project's root directory to the Python path to allow for `src` imports\n",
    "# This path is relative to the notebook's location: notebooks/01_data_exploration/\n",
    "project_root = os.path.abspath(os.path.join('..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "from src.data.loaders import CaliforniaHousingLoader\n",
    "\n",
    "try:\n",
    "    # Instantiate and load the data\n",
    "    loader = CaliforniaHousingLoader()\n",
    "    housing_df = loader.load()\n",
    "    print(\"Data loaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during data loading: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Inspection\n",
    "\n",
    "**Reasoning:** Before any deep analysis, we perform a first-pass inspection of the DataFrame. This helps us quickly understand its basic characteristics: the shape, data types, and presence of missing values. It's the quickest way to get a feel for the data we're working with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first few rows to get a visual sense of the data\n",
    "housing_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a concise summary of the dataframe\n",
    "# This is crucial for checking data types and missing values.\n",
    "housing_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initial Observations from `.info()`:**\n",
    "- The dataset contains 20,640 entries.\n",
    "- There are 8 features and 1 target variable (`price`).\n",
    "- All columns are `float64`, which is expected for this dataset.\n",
    "- **Crucially, there are no missing values.** This simplifies our preprocessing, but in a real-world scenario, we would need a strategy for handling them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate descriptive statistics\n",
    "# This gives us a sense of the scale, central tendency, and spread of each feature.\n",
    "housing_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from `.describe()`:**\n",
    "- **Varying Scales:** The scales of the features vary widely (e.g., `MedInc` is in single digits, while `Population` is in thousands). This strongly suggests that **feature scaling will be essential** for distance-based algorithms (like SVMs) and algorithms that use regularization (like Ridge/Lasso).\n",
    "- **Capping:** `HouseAge` and the target `price` have a max value (52.0 and 5.00001 respectively) that appears to be a cap. This is an artifact of data collection and could impact model performance. We must investigate this.\n",
    "- **Potential Outliers/Skew:** The `AveRooms`, `AveBedrms`, `Population`, and `AveOccup` features have a large difference between their 75th percentile and max values, indicating the presence of outliers or a highly skewed distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Target Variable Analysis (`price`)\n",
    "\n",
    "**Reasoning:** In any supervised learning task, understanding the distribution of the target variable is the most important step. Its characteristics (skewness, outliers, range) directly influence model choice, evaluation metrics, and potential transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(housing_df['price'], kde=True, bins=50)\n",
    "plt.title('Distribution of Median House Value (Price)')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=housing_df['price'])\n",
    "plt.title('Box Plot of Median House Value (Price)')\n",
    "plt.xlabel('Median House Value ($100,000s)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Target Variable:**\n",
    "- The distribution is **right-skewed**. Many models perform better on normally distributed targets.\n",
    "- There is a clear **capping** at the maximum value of 5. This is a significant data artifact. The model may struggle to predict prices for houses in this top tier. This needs a dedicated strategy in the feature engineering phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Distribution and Relationships\n",
    "\n",
    "**Reasoning:** Now we analyze the features. We'll look at their individual distributions (univariate analysis) to spot skewness or other patterns. Then, we'll examine their relationship with the target variable (bivariate analysis) to identify which features are most predictive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms for all numerical features to see their distributions\n",
    "housing_df.hist(bins=50, figsize=(20, 15))\n",
    "plt.suptitle('Histograms of All Numerical Features', y=0.92)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Feature Distributions:**\n",
    "- **Skewness:** `MedInc`, `AveRooms`, `AveBedrms`, `Population`, and `AveOccup` are all heavily right-skewed. A log transformation might help normalize these distributions, which can be beneficial for linear models.\n",
    "- **Capping:** `HouseAge` is also clearly capped at 52 years.\n",
    "- **Geographical Features:** `Latitude` and `Longitude` show multiple peaks, which makes sense as they represent geographical locations with population clusters (e.g., Los Angeles and Bay Area)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correlation Analysis\n",
    "\n",
    "**Reasoning:** A correlation matrix is a fast way to quantify the linear relationships between features. We are most interested in the correlations with our target, `price`. This helps us identify the most promising features and also spot potential multicollinearity (features that are highly correlated with each other)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing_df.corr()\n",
    "\n",
    "# Focus on correlations with the target variable\n",
    "print(\"Correlation with Target (price):\")\n",
    "print(corr_matrix['price'].sort_values(ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the full correlation matrix with a heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Housing Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations on Correlations:**\n",
    "- **`MedInc` (Median Income)** has the strongest positive correlation with `price` (0.69). This is highly intuitive and makes it our most important feature.\n",
    "- **`AveRooms`** has a weak positive correlation (0.15).\n",
    "- **`Latitude`** has a slight negative correlation (-0.14), suggesting that houses in the north are slightly cheaper, though this is a very weak signal on its own.\n",
    "- **Multicollinearity:** There is a high correlation between `AveRooms` and `AveBedrms` (0.85). This might be an issue for interpreting coefficients in linear models, but is less of a concern for tree-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Geospatial Analysis\n",
    "\n",
    "**Reasoning:** Since we have latitude and longitude data, we can create a geographical scatter plot. This is a powerful visualization that can reveal patterns that are impossible to see in tables or histograms. We can check if price is related to location, like proximity to the coast or major cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_df.plot(kind=\"scatter\", x=\"Longitude\", y=\"Latitude\", alpha=0.4,\n",
    "                s=housing_df[\"Population\"]/100, label=\"Population\", figsize=(12,9),\n",
    "                c=\"price\", cmap=plt.get_cmap(\"jet\"), colorbar=True,\n",
    "                sharex=False)\n",
    "plt.title(\"California Housing Prices and Population Density\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations from Geospatial Plot:**\n",
    "- The plot clearly resembles a map of California.\n",
    "- **High-priced areas (red/yellow)** are concentrated along the coast, particularly in the Bay Area (around Longitude -122) and Southern California (around Los Angeles and San Diego).\n",
    "- Inland areas generally have lower prices (blue/green).\n",
    "- This visualization confirms that **location is a critical factor** in determining house prices. This suggests that creating location-based features (e.g., distance to coast, clustering of districts) could be highly beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary of Findings & Next Steps\n",
    "\n",
    "**Reasoning:** The final step of EDA is to synthesize all our findings into a concise summary and, most importantly, to define a clear action plan for the next phase of the project: Feature Engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings:\n",
    "1.  **Data Quality:** The data is clean with no missing values, but features have vastly different scales.\n",
    "2.  **Target Variable:** `price` is right-skewed and **capped** at \\$500,000. This capping is a major characteristic that must be addressed.\n",
    "3.  **Feature Skewness:** Several important features are highly skewed (`MedInc`, `AveRooms`, etc.).\n",
    "4.  **Key Predictor:** `MedInc` is by far the strongest linear predictor of `price`.\n",
    "5.  **Location Importance:** Geospatial data confirms that prices are heavily dependent on location, especially proximity to the coast and major urban centers.\n",
    "6.  **Multicollinearity:** `AveRooms` and `AveBedrms` are highly correlated.\n",
    "\n",
    "### Proposed Next Steps (for Feature Engineering):\n",
    "1.  **Transformation:** Apply log transformations to the skewed features (e.g., `MedInc`, `Population`, `AveRooms`, `AveOccup`) to make their distributions more normal, which can help linear models.\n",
    "2.  **Feature Scaling:** Standardize or normalize all features to bring them to a common scale. This is **required** for many ML algorithms.\n",
    "3.  **Feature Creation:**\n",
    "    - Create new combination features that might capture more signal, such as `rooms_per_person` or `bedrooms_per_room`.\n",
    "    - Engineer location-based features. We could use clustering on `Latitude` and `Longitude` to create a `location_category` feature, or calculate `distance_to_coast`.\n",
    "4.  **Handling Capped Values:** Decide on a strategy for the capped `price` and `HouseAge`. For the target `price`, options include:\n",
    "    a) Removing these instances before training.\n",
    "    b) Leaving them as is, but being aware that the model will not be able to predict values above the cap.\n",
    "    c) Treating it as a classification problem for that price bracket (more complex).\n",
    "\n",
    "This structured analysis provides a clear, evidence-based path forward for the `02_feature_engineering` phase."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
